{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8428617,"sourceType":"datasetVersion","datasetId":5019093},{"sourceId":8431186,"sourceType":"datasetVersion","datasetId":5021075}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<font size = 5>**This is a jupyter notebook for kaggle to train DDPM+CFG+EMA+DDIM**","metadata":{}},{"cell_type":"code","source":"%%writefile submodules.py\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DoubleConv(nn.Module):\n    def __init__(self,in_channels,out_channels,mid_channels=None,residual=False):\n        super().__init__()\n        self.residual = residual\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels,mid_channels, kernel_size = 3,padding = 1, bias = False),\n            nn.GroupNorm(1,mid_channels),\n            nn.GELU(),\n            nn.Conv2d(mid_channels,out_channels, kernel_size = 3,padding = 1, bias = False),\n            nn.GroupNorm(1,out_channels),\n        )\n    \n    def forward(self,x):\n        if self.residual:\n            return F.gelu(x + self.double_conv(x))\n        else:\n            return self.double_conv(x)\nclass Down(nn.Module):\n    def __init__(self,in_channels,out_channels, emb_dim = 256):\n        super().__init__()\n        self.maxpool_conv  = nn.Sequential(\n        nn.MaxPool2d(2),\n        DoubleConv(in_channels,in_channels,residual=True),\n        DoubleConv(in_channels,out_channels),\n        )\n        self.emb_layer = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(\n                emb_dim,\n                out_channels,\n            ),)\n        \n    def forward(self,x,t):\n        x = self.maxpool_conv(x)\n        emb = self.emb_layer(t)[:,:,None,None].repeat(1,1,x.shape[-2],x.shape[-1])\n        return x+emb\n\nclass Up(nn.Module):\n    def __init__(self,in_channels,out_channels, emb_dim = 256):\n        super().__init__()\n        self.up = nn.Upsample(scale_factor=2,mode=\"bilinear\",align_corners=True)\n        self.conv = nn.Sequential(\n            DoubleConv(in_channels,in_channels,residual=True),\n            DoubleConv(in_channels,out_channels,in_channels//2),\n        )\n        self.emb_layer = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(\n                emb_dim,\n                out_channels,\n            ),)\n        \n    def forward(self,x,skip_x,t):\n        x = self.up(x)\n        x = torch.cat([skip_x,x], dim = 1)\n        x = self.conv(x)\n        emb = self.emb_layer(t)[:,:,None,None].repeat(1,1,x.shape[-2],x.shape[-1])\n        return x+emb\n    \nclass SelfAttention(nn.Module):\n    def __init__(self,channels,size):\n        super(SelfAttention,self).__init__()\n        self.channels = channels\n        self.size = size\n        self.mha = nn.MultiheadAttention(channels,4,batch_first=True)\n        self.ln = nn.LayerNorm([channels])\n        self.ff_self = nn.Sequential(\n            nn.LayerNorm([channels]),\n            nn.Linear(channels,channels),\n            nn.GELU(),\n            nn.Linear(channels,channels),\n        )\n        \n    def forward(self,x):\n        x = x.view(-1,self.channels,self.size * self.size).swapaxes(1,2)\n        x_ln = self.ln(x)\n        attn_value,_ = self.mha(x_ln,x_ln,x_ln)\n        attn_value = attn_value + x\n        attn_value = self.ff_self(attn_value) + attn_value\n        return attn_value.swapaxes(2,1).view(-1,self.channels,self.size,self.size)\n    \n    \n\nclass EMA:\n    def __init__(self, beta):\n        super().__init__()\n        self.beta = beta\n        self.step = 0\n\n    def update_model_average(self, ma_model, current_model):\n        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n            old_weight, up_weight = ma_params.data, current_params.data\n            ma_params.data = self.update_average(old_weight, up_weight)\n\n    def update_average(self, old, new):\n        if old is None:\n            return new\n        return old * self.beta + (1 - self.beta) * new\n\n    def step_ema(self, ema_model, model, step_start_ema=2000):\n        if self.step < step_start_ema:\n            self.reset_parameters(ema_model, model)\n            self.step += 1\n            return\n        self.update_model_average(ema_model, model)\n        self.step += 1\n\n    def reset_parameters(self, ema_model, model):\n        ema_model.load_state_dict(model.state_dict())","metadata":{"execution":{"iopub.status.busy":"2024-05-16T08:13:30.568100Z","iopub.execute_input":"2024-05-16T08:13:30.568890Z","iopub.status.idle":"2024-05-16T08:13:30.584188Z","shell.execute_reply.started":"2024-05-16T08:13:30.568857Z","shell.execute_reply":"2024-05-16T08:13:30.583169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile UNet.py\nimport os\nimport torch\nimport torch.nn as nn\nfrom submodules import DoubleConv\nfrom submodules import Down\nfrom submodules import SelfAttention\nfrom submodules import Up\n\n\nclass UNet(nn.Module):\n    def __init__(self,c_in=3,c_out=3,time_dim=256,device=\"cuda\"):\n        super().__init__()\n        self.device = device\n        self.time_dim = time_dim\n        self.inc = DoubleConv(c_in,64)\n        \n        self.down1 = Down(64,128) # input_channel, output_channel\n        self.sa1 = SelfAttention(128,32) # channel, img resolution\n        \n        self.down2 = Down(128,256)\n        self.sa2 = SelfAttention(256,16)\n        \n        self.down3 = Down(256,256)\n        self.sa3 = SelfAttention(256,8)\n        \n        # bottle neck\n        self.bot1 = DoubleConv(256,512)\n        self.bot2 = DoubleConv(512,512)\n        self.bot3 = DoubleConv(512,256)\n        \n        self.up1 = Up(512,128)\n        self.sa4 = SelfAttention(128,16)\n        \n        self.up2 = Up(256,64)\n        self.sa5 = SelfAttention(64,32)\n        \n        self.up3 = Up(128,64)\n        self.sa6 = SelfAttention(64,64)\n        \n        self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n        \n    def pos_encoding(self,t,channels):\n        inv_freq = 1.0/ (\n            10000\n            ** (torch.arange(0,channels,2,device=self.device).float()/channels)\n        )\n        pos_enc_a = torch.sin(t.repeat(1,channels // 2) * inv_freq)\n        pos_enc_b = torch.cos(t.repeat(1,channels // 2) * inv_freq)\n        pos_enc = torch.cat([pos_enc_a,pos_enc_b],dim=-1)\n        return pos_enc\n    \n    def forward(self,x,t):\n        t = t.unsqueeze(-1).type(torch.float)\n        t = self.pos_encoding(t,self.time_dim)\n        \n        x1 = self.inc(x)\n        \n        x2 = self.down1(x1,t)\n        x2 = self.sa1(x2)\n        \n        x3 = self.down2(x2,t)\n        x3 = self.sa2(x3)\n        \n        x4 = self.down3(x3,t)\n        x4 = self.sa3(x4)\n        \n        x4 = self.bot1(x4)\n        x4 = self.bot2(x4)\n        x4 = self.bot3(x4)\n        \n        x = self.up1(x4,x3,t)\n        x = self.sa4(x)\n        \n        x = self.up2(x,x2,t)\n        x = self.sa5(x)\n        \n        x = self.up3(x,x1,t)\n        x = self.sa6(x)\n        \n        output = self.outc(x)\n        return output\n    \n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-16T08:14:29.325512Z","iopub.execute_input":"2024-05-16T08:14:29.325877Z","iopub.status.idle":"2024-05-16T08:14:29.332931Z","shell.execute_reply.started":"2024-05-16T08:14:29.325849Z","shell.execute_reply":"2024-05-16T08:14:29.331951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile UNet_conditional.py\nimport os\nimport torch\nimport torch.nn as nn\nfrom submodules import DoubleConv\nfrom submodules import Down\nfrom submodules import SelfAttention\nfrom submodules import Up\n\n\nclass UNet_conditional(nn.Module):\n    def __init__(self,c_in=3,c_out=3,time_dim=256,num_classes=None,device=\"cuda\"):\n        super().__init__()\n        self.device = device\n        self.time_dim = time_dim\n        self.inc = DoubleConv(c_in,64)\n        \n        self.down1 = Down(64,128) # input_channel, output_channel\n        self.sa1 = SelfAttention(128,32) # channel, img resolution\n        \n        self.down2 = Down(128,256)\n        self.sa2 = SelfAttention(256,16)\n        \n        self.down3 = Down(256,256)\n        self.sa3 = SelfAttention(256,8)\n        \n        # bottle neck\n        self.bot1 = DoubleConv(256,512)\n        self.bot2 = DoubleConv(512,512)\n        self.bot3 = DoubleConv(512,256)\n        \n        self.up1 = Up(512,128)\n        self.sa4 = SelfAttention(128,16)\n        \n        self.up2 = Up(256,64)\n        self.sa5 = SelfAttention(64,32)\n        \n        self.up3 = Up(128,64)\n        self.sa6 = SelfAttention(64,64)\n        \n        self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n        \n        if num_classes is not None:\n            self.label_emb = nn.Embedding(num_classes,time_dim)\n            \n        \n    def pos_encoding(self,t,channels):\n        inv_freq = 1.0/ (\n            10000\n            ** (torch.arange(0,channels,2,device=self.device).float()/channels)\n        )\n        pos_enc_a = torch.sin(t.repeat(1,channels // 2) * inv_freq)\n        pos_enc_b = torch.cos(t.repeat(1,channels // 2) * inv_freq)\n        pos_enc = torch.cat([pos_enc_a,pos_enc_b],dim=-1)\n        return pos_enc\n    \n    def forward(self,x,t,y=None):\n        t = t.unsqueeze(-1).type(torch.float)\n        t = self.pos_encoding(t,self.time_dim)\n        if y is not None:\n            t += self.label_emb(y)\n        \n        x1 = self.inc(x)\n        \n        x2 = self.down1(x1,t)\n        x2 = self.sa1(x2)\n        \n        x3 = self.down2(x2,t)\n        x3 = self.sa2(x3)\n        \n        x4 = self.down3(x3,t)\n        x4 = self.sa3(x4)\n        \n        x4 = self.bot1(x4)\n        x4 = self.bot2(x4)\n        x4 = self.bot3(x4)\n        \n        x = self.up1(x4,x3,t)\n        x = self.sa4(x)\n        \n        x = self.up2(x,x2,t)\n        x = self.sa5(x)\n        \n        x = self.up3(x,x1,t)\n        x = self.sa6(x)\n        \n        output = self.outc(x)\n        return output\n    \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-16T08:14:32.068074Z","iopub.execute_input":"2024-05-16T08:14:32.068430Z","iopub.status.idle":"2024-05-16T08:14:32.075554Z","shell.execute_reply.started":"2024-05-16T08:14:32.068401Z","shell.execute_reply":"2024-05-16T08:14:32.074550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n!rm -r /kaggle/working/utils.py\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T08:14:35.084797Z","iopub.execute_input":"2024-05-16T08:14:35.085456Z","iopub.status.idle":"2024-05-16T08:14:36.030499Z","shell.execute_reply.started":"2024-05-16T08:14:35.085420Z","shell.execute_reply":"2024-05-16T08:14:36.029359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile utils.py\n\nimport os\nimport torch\nimport torchvision\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom torch import optim\nfrom UNet import UNet\nimport torch.nn as nn\nimport logging\n\n\ndef plot_images(images):\n    plt.figure(figsize = (32,32) )\n    plt.imshow(torch.cat(\n        [\n            torch.cat([i for i in images.cpu()],dim=-1)\n        ],dim=-2).permute(1,2,0).cpu()\n        )\n    plt.show()\n\ndef save_images(images,path,**kwargs):\n    grid = torchvision.utils.make_grid(images,**kwargs)\n    ndarr = grid.permute(1,2,0).to(\"cpu\").numpy()\n    im = Image.fromarray(ndarr)\n    im.save(path)\n    \ndef get_data(args):\n    transforms = torchvision.transforms.Compose([\n        torchvision.transforms.Resize(80),\n        torchvision.transforms.RandomResizedCrop(args.img_size,scale=(0.8,1.0)),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n    ])\n    dataset = torchvision.datasets.ImageFolder(args.dataset_path,transform=transforms)\n    \n    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n    return dataloader\n\ndef setup_logging(run_name):\n    os.makedirs(\"models\",exist_ok=True)\n    os.makedirs(\"results\",exist_ok=True)\n    os.makedirs(os.path.join(\"models\",run_name),exist_ok=True)\n    os.makedirs(os.path.join(\"results\",run_name),exist_ok=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T08:14:37.028844Z","iopub.execute_input":"2024-05-16T08:14:37.029759Z","iopub.status.idle":"2024-05-16T08:14:37.036343Z","shell.execute_reply.started":"2024-05-16T08:14:37.029723Z","shell.execute_reply":"2024-05-16T08:14:37.035318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/\n!rm ddpm.py\n!rm -r /kaggle/working/results/DDPM_conditional","metadata":{"execution":{"iopub.status.busy":"2024-05-16T10:48:00.477793Z","iopub.execute_input":"2024-05-16T10:48:00.478513Z","iopub.status.idle":"2024-05-16T10:48:02.440924Z","shell.execute_reply.started":"2024-05-16T10:48:00.478474Z","shell.execute_reply":"2024-05-16T10:48:02.439583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile ddpm.py\n\n\nimport os\nimport copy\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom matplotlib import pyplot as plt\nfrom torch import optim\nfrom tqdm import tqdm\nimport logging\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom utils import plot_images\nfrom utils import save_images\nfrom utils import get_data\nfrom utils import setup_logging\n\n# from UNet import UNet\nfrom UNet_conditional import UNet_conditional\nfrom submodules import EMA\n\nfrom accelerate import Accelerator\nfrom accelerate.utils import DistributedDataParallelKwargs\nimport torch.nn.functional as F\n\nlogging.basicConfig(format=\"%(asctime)s-%(levelname)s: %(message)s\",level=logging.INFO,datafmt=\"%I:%M:%S\")\n\nfrom collections import OrderedDict\n\ndef extract(v, t, x_shape):\n    # v[T]\n    # t[B] x_shape = [B,C,H,W]\n    out = torch.gather(v, index=t, dim=0).float()\n    # [B,1,1,1],分别代表batch_size,通道数,长,宽\n    return out.view([t.shape[0]] + [1] * (len(x_shape) - 1))\n\nclass Diffusion:\n    def __init__(self,noise_steps=1000,beta_start=1e-4,beta_end=0.02,img_size=64,device=\"cuda\"):\n        self.noise_steps = noise_steps\n        self.beta_start = beta_start\n        self.beta_end = beta_end\n        self.img_size = img_size\n        self.device = device\n#         self.beta = self.prepare_noise_schedule()\n        self.beta = self.prepare_noise_schedule().to(device) # shape: (1000,)\n        self.alpha = 1. - self.beta                       # shape: (1000,)\n        self.alpha_hat = torch.cumprod(self.alpha,dim=0) # (1-β_i)(1-β_i+1)... shape: (1000,)\n        # DDIM\n        self.sqrt_recip_alphas_bar = torch.sqrt(1. / self.alpha_hat)\n        self.sqrt_recipm1_alphas_bar = torch.sqrt(1. / self.alpha_hat - 1)\n        \n        self.alphas_bar_prev_whole = F.pad(self.alpha_hat, [1, 0], value=1)\n        self.ddim_eta = 0 \n        \n    def prepare_noise_schedule(self):\n        return torch.linspace(self.beta_start,self.beta_end,self.noise_steps)\n    \n    def noise_images(self,x,t):\n        \"\"\"\n        x: images, shape:\n        t: time_steps, shape:\n        \"\"\"\n        sqrt_alpha_hat  = torch.sqrt(self.alpha_hat[t])[:,None,None,None]\n        sqrt_one_minus_alpha_hat = torch.sqrt(1-self.alpha_hat[t])[:,None,None,None]\n        sigma = torch.randn_like(x)\n        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * sigma, sigma\n    \n    def sample_timesteps(self,n):\n        return torch.randint(low=1,high = self.noise_steps,size=(n,)) # rand int value\n    \n    def predict_xstart_from_eps(self, x_t, t, eps):\n        return (\n            extract(self.sqrt_recip_alphas_bar, t, x_t.shape) * x_t -\n            extract(self.sqrt_recipm1_alphas_bar, t, x_t.shape) * eps\n        )\n    \n#     def sample(self,model,n):\n    def sample(self,model,n,labels,cfg_scale=3):\n        \"\"\"\n        n=?\n        \"\"\"\n        logging.info(f\"sampling {n} new images... \")\n        model.eval()\n        with torch.no_grad():\n#              x = torch.randn((n,3,self.img_size,self.img_size)).to(self.device) # shape:n,3,h,w\n#             for i in tqdm(reversed(range(1,self.noise_steps)),position=0):\n#                 t = (torch.ones(n)*i).long().to(self.device) # shape:(bs,) value:(i,i,......) like 1000,1000,1000\n# #                 predicted_noise = model(x,t)\n#                 predicted_noise = model(x,t,labels)\n#                 if cfg_scale>0:\n#                     uncond_predicted_noise = model(x, t, None)\n#                     predicted_noise = torch.lerp(uncond_predicted_noise,predicted_noise,cfg_scale)\n                \n#                 alpha = self.alpha[t][:,None,None,None] # only scalar value like 1000:(1,1,1,1)\n#                 alpha_hat = self.alpha_hat[t][:,None,None,None]\n#                 beta = self.beta[t][:,None,None,None]\n#                 if i>1:\n#                     noise = torch.randn_like(x) # noise z\n#                 else:\n#                     noise = torch.zeros_like(x)\n                    \n#                 x = 1/torch.sqrt(alpha)*(x-((1-alpha)/(torch.sqrt(1-alpha_hat)))*predicted_noise)+torch.sqrt(beta)*noise\n          # try DDIM:\n            sample_steps = 5 # faster 10 \n            t_seq = torch.arange(sample_steps, self.noise_steps + 1, sample_steps) # x_t\n            t_prev_seq = t_seq - sample_steps # x_{prev}\n            \n            x_t = torch.randn((n,3,self.img_size,self.img_size)).to(self.device) # shape:n,3,h,w\n            for i, j in tqdm(zip(reversed(list(t_seq)), reversed(list(t_prev_seq))), desc='Inference'):\n                t = x_t.new_ones([x_t.shape[0], ], dtype=torch.long) * i\n                prev_t = x_t.new_ones([x_t.shape[0], ], dtype=torch.long) * j\n                alpha_cumprod_t = extract(self.alphas_bar_prev_whole, t, x_t.shape)\n                alpha_cumprod_t_prev = extract(self.alphas_bar_prev_whole, prev_t, x_t.shape)\n                \n                eps = model(x_t, t - 1,labels) # 采用t-1是因为原本的ddpm的0位置元素代表t=1时刻,差了一个1\n                # 计算x_0,用于第一项\n                x_0 = self.predict_xstart_from_eps(x_t, t - 1, eps)\n#                 if self.clip_denoised:\n#                     x_0 = torch.clamp(x_0, min=-1., max=1.) # 裁剪梯度\n                if cfg_scale>0:\n                     uncond_predicted_noise = model(x_t, t-1, None)\n                     predicted_noise = torch.lerp(uncond_predicted_noise,eps,cfg_scale)\n                \n                sigma_t = self.ddim_eta * torch.sqrt(\n                    (1 - alpha_cumprod_t_prev) / (1 - alpha_cumprod_t) * (1 - alpha_cumprod_t / alpha_cumprod_t_prev))\n                \n                pred_dir_xt = torch.sqrt(1 - alpha_cumprod_t_prev - sigma_t ** 2) * eps\n                if j>1:\n                     noise = torch.randn_like(x_t) # noise z\n                else:\n                     noise = torch.zeros_like(x_t)\n                        \n                x_prev = torch.sqrt(alpha_cumprod_t_prev) * x_0 + pred_dir_xt + sigma_t ** 2 * noise\n                x_t = x_prev               \n        model.train()\n        x = (x_t.clamp(-1,1)+1)/2 # normalize\n        x = (x * 255).type(torch.uint8) # factor pixels\n                \n        return x\n    \ndef train(args):\n    setup_logging(args.run_name)\n    \n\n\n    kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n    accelerator = Accelerator(kwargs_handlers=[kwargs])\n\n    \n#     accelerator = Accelerator()\n    \n    device = args.device\n    dataloader = get_data(args)\n    \n    model = UNet_conditional(num_classes = args.num_classes)\n    \n    if args.resumes:\n        ckpts = torch.load(args.resumes)\n        new_state_dict = OrderedDict()\n        for k, v in ckpts.items():\n            name = k[7:]  # 移除'module.'\n            new_state_dict[name] = v\n        model.load_state_dict(new_state_dict)\n        print(\"resume epoch success!\\n\")\n        \n#     model = UNet()\n#     model = UNet().to(device)\n    optimizer = optim.AdamW(model.parameters(),lr= args.lr)\n    mse = nn.MSELoss()\n    diffusion = Diffusion(img_size = args.img_size,device=device)\n    logger = SummaryWriter(os.path.join(\"runs\",args.run_name))\n    l = len(dataloader)\n    \n    \n    model,optimizer,dataloader = accelerator.prepare(model,optimizer,dataloader)\n    args.accelerator = accelerator\n    \n    ema = EMA(beta=0.995)\n    ema_model = copy.deepcopy(model).eval().requires_grad_(False)\n    \n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_drop, gamma=0.1)\n    \n    for epoch in range(args.epochs):\n        logging.info(f\"starting epoch {epoch}: \")\n#         print(f\"starting epoch {epoch}: \")\n        pbar = tqdm(dataloader)\n#         for i,(images,_) in enumerate(pbar):\n        for i,(images,labels) in enumerate(pbar):\n    \n            images = images.to(device)\n            \n            labels = labels.to(device)\n            \n#             images = images\n#             t = diffusion.sample_timesteps(images.shape[0])\n            t = diffusion.sample_timesteps(images.shape[0]).to(device)\n\n            x_t,noise = diffusion.noise_images(images,t)\n#         predicted_noise = model(x_t,t),\n            if np.random.random()<0.1:\n                labels = None # cfg,10% unconditional\n            predicted_noise = model(x_t,t,labels)\n    \n            loss = mse(noise,predicted_noise)\n            \n            optimizer.zero_grad()\n#             loss.backward()\n            args.accelerator.backward(loss)\n            optimizer.step()\n            ema.step_ema(ema_model,model)\n            scheduler.step()\n            \n            pbar.set_postfix(MSE=loss.item())\n            logger.add_scalar(\"MSE\",loss.item(),global_step= epoch*l+i)\n        if epoch>=200:    \n            if (epoch+1) % 10 == 0:\n                labels = torch.arange(args.num_classes).long().to(device)\n\n                sampled_images = diffusion.sample(model, n=len(labels), labels=labels)\n                sampled_images = args.accelerator.gather_for_metrics(sampled_images)\n                save_images(sampled_images, os.path.join(\"results\",args.run_name,f\"{epoch}.jpg\"))\n\n                ema_sampled_images = diffusion.sample(ema_model, n=len(labels), labels=labels)\n                ema_sampled_images = args.accelerator.gather_for_metrics(ema_sampled_images)\n                save_images(ema_sampled_images, os.path.join(\"results\", args.run_name, f\"{epoch}_ema.jpg\"))\n\n                torch.save(model.state_dict(),os.path.join(\"models\",args.run_name,f\"{epoch}.ckpts\"))\n                torch.save(ema_model.state_dict(), os.path.join(\"models\", args.run_name, f\"ema_ckpt.pt\"))\n                torch.save(optimizer.state_dict(), os.path.join(\"models\", args.run_name, f\"optim_{epoch}.pt\"))\n            \n    \ndef launch():\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-f\",\"--file\",default=\"file\")\n    \n    \n    \n    \n    args = parser.parse_args()\n    args.resumes = None # if you want to resume your ckpts\n    args.run_name = \"DDPM_conditional\"\n    args.num_classes = 10   # change your datasets classes num\n    args.epochs = 500\n    args.batch_size = 8\n    args.img_size = 64\n    args.dataset_path = r\"/kaggle/input/ddpm-nwpu-instance/content/train_nwpu\" # change your datasets file path\n    args.device = \"cuda\"\n    args.lr = 2e-4\n    args.lr_drop = 450\n    train(args)\n    \nif __name__==\"__main__\":\n    launch()","metadata":{"execution":{"iopub.status.busy":"2024-05-16T10:48:04.157384Z","iopub.execute_input":"2024-05-16T10:48:04.157958Z","iopub.status.idle":"2024-05-16T10:48:04.172275Z","shell.execute_reply.started":"2024-05-16T10:48:04.157923Z","shell.execute_reply":"2024-05-16T10:48:04.171264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# os.environ['TORCH_DISTRIBUTED_DEBUG'] = 'INFO'","metadata":{"execution":{"iopub.status.busy":"2024-05-16T08:14:53.725340Z","iopub.execute_input":"2024-05-16T08:14:53.726285Z","iopub.status.idle":"2024-05-16T08:14:53.730598Z","shell.execute_reply.started":"2024-05-16T08:14:53.726245Z","shell.execute_reply":"2024-05-16T08:14:53.729530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-05-16T13:04:20.525647Z","iopub.execute_input":"2024-05-16T13:04:20.526549Z","iopub.status.idle":"2024-05-16T13:04:20.724600Z","shell.execute_reply.started":"2024-05-16T13:04:20.526517Z","shell.execute_reply":"2024-05-16T13:04:20.723533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!accelerate launch --multi_gpu ddpm.py  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}